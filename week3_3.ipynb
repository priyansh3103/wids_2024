{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving Direct OLSLR using BFGS...\n",
      "Iteration 0, Gradient Norm: 5376.846925988702, Denominator: [[1.59216584e+11]]\n",
      "Iteration 10, Gradient Norm: 7.659055707878938e+74, Denominator: [[7.09181732e+160]]\n",
      "Iteration 20, Gradient Norm: 5.2459882499749e+149, Denominator: [[nan]]\n",
      "Iteration 30, Gradient Norm: nan, Denominator: [[nan]]\n",
      "Iteration 40, Gradient Norm: nan, Denominator: [[nan]]\n",
      "Iteration 50, Gradient Norm: nan, Denominator: [[nan]]\n",
      "Iteration 60, Gradient Norm: nan, Denominator: [[nan]]\n",
      "Iteration 70, Gradient Norm: nan, Denominator: [[nan]]\n",
      "Iteration 80, Gradient Norm: nan, Denominator: [[nan]]\n",
      "Iteration 90, Gradient Norm: nan, Denominator: [[nan]]\n",
      "x_star_f:\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "Number of iterations: 100\n",
      "\n",
      "Observations:\n",
      "Direct OLSLR using BFGS with stabilization should now avoid NaN or overflow errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/1365957882.py:46: RuntimeWarning: overflow encountered in matmul\n",
      "  denom_1 = max(s.T @ y_grad, epsilon)\n",
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/1365957882.py:46: RuntimeWarning: invalid value encountered in matmul\n",
      "  denom_1 = max(s.T @ y_grad, epsilon)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and scale the dataset\n",
    "digits = load_digits()\n",
    "A = digits.data\n",
    "y = digits.target.reshape(-1, 1)\n",
    "\n",
    "# Standardize the data matrix A\n",
    "scaler = StandardScaler()\n",
    "A = scaler.fit_transform(A)\n",
    "\n",
    "# Define the gradient for Direct OLSLR\n",
    "def direct_ols_gradient(A, x, y):\n",
    "    return A.T @ (A @ x - y)\n",
    "\n",
    "# BFGS Implementation for Direct OLSLR\n",
    "def bfgs_direct_ols(A, y, tol=1e-6, max_iter=100, epsilon=1e-8, regularization=1e-5):\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros((n, 1))  # Starting point\n",
    "    B = np.eye(n)  # Initial Hessian approximation\n",
    "    history = []\n",
    "\n",
    "    # Regularization for improved stability\n",
    "    regularization_matrix = regularization * np.eye(n)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad = direct_ols_gradient(A, x, y)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "        # Regularized Hessian for better stability\n",
    "        hess_approx = A.T @ A + regularization_matrix\n",
    "\n",
    "        # Compute search direction\n",
    "        p = -np.linalg.solve(B, grad)\n",
    "        x_new = x + p\n",
    "\n",
    "        s = x_new - x\n",
    "        y_grad = direct_ols_gradient(A, x_new, y) - grad\n",
    "\n",
    "        # Safeguard for small denominators\n",
    "        denom_1 = max(s.T @ y_grad, epsilon)\n",
    "        denom_2 = max(s.T @ (B @ s), epsilon)\n",
    "\n",
    "        # Ensure s and y_grad are column vectors\n",
    "        s = s.reshape(-1, 1)\n",
    "        y_grad = y_grad.reshape(-1, 1)\n",
    "\n",
    "        # Update B using the stabilized BFGS formula\n",
    "        Bs = B @ s\n",
    "        B += np.outer(s, s) / denom_1 - np.outer(Bs, Bs) / denom_2\n",
    "\n",
    "        # Update x and save history\n",
    "        x = x_new\n",
    "        history.append(np.linalg.norm(grad))\n",
    "\n",
    "        # Debugging: Print intermediate values\n",
    "        if k % 10 == 0:  # Print every 10 iterations\n",
    "            print(f\"Iteration {k}, Gradient Norm: {np.linalg.norm(grad)}, Denominator: {denom_1}\")\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Solve Direct OLSLR using BFGS\n",
    "print(\"Solving Direct OLSLR using BFGS...\")\n",
    "x_star_f, history_f = bfgs_direct_ols(A, y)\n",
    "print(f\"x_star_f:\\n{x_star_f}\")\n",
    "print(f\"Number of iterations: {len(history_f)}\")\n",
    "\n",
    "# Observations\n",
    "print(\"\\nObservations:\")\n",
    "print(\"Direct OLSLR using BFGS with stabilization should now avoid NaN or overflow errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving Regularized OLSLR using BFGS (lambda = 0.001)...\n",
      "Iteration 0\n",
      "Gradient Norm: 5376.846925988702\n",
      "s.T @ y_grad: [[1.59213429e+11]]\n",
      "Iteration 10\n",
      "Gradient Norm: 4.729104087919709e+74\n",
      "s.T @ y_grad: [[2.42909221e+160]]\n",
      "Iteration 20\n",
      "Gradient Norm: 1.897105965587677e+149\n",
      "s.T @ y_grad: [[inf]]\n",
      "Iteration 30\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "Iteration 40\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "Iteration 50\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "Iteration 60\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "Iteration 70\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "Iteration 80\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "Iteration 90\n",
      "Gradient Norm: nan\n",
      "s.T @ y_grad: [[nan]]\n",
      "x_star_f_lambda:\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "Number of iterations: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/2414925323.py:46: RuntimeWarning: overflow encountered in matmul\n",
      "  print(f\"s.T @ y_grad: {s.T @ y_grad}\")\n",
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/2414925323.py:49: RuntimeWarning: overflow encountered in matmul\n",
      "  denom_1 = max(s.T @ y_grad, epsilon)\n",
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/2414925323.py:49: RuntimeWarning: invalid value encountered in matmul\n",
      "  denom_1 = max(s.T @ y_grad, epsilon)\n",
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/2414925323.py:50: RuntimeWarning: overflow encountered in matmul\n",
      "  denom_2 = max(s.T @ (B @ s), epsilon)\n",
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_97064/2414925323.py:50: RuntimeWarning: invalid value encountered in matmul\n",
      "  denom_2 = max(s.T @ (B @ s), epsilon)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/numeric.py:925: RuntimeWarning: overflow encountered in multiply\n",
      "  return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and scale the dataset\n",
    "digits = load_digits()\n",
    "A = digits.data\n",
    "y = digits.target.reshape(-1, 1)\n",
    "\n",
    "# Standardize the data matrix A\n",
    "scaler = StandardScaler()\n",
    "A = scaler.fit_transform(A)\n",
    "\n",
    "# Define the gradient for Regularized OLSLR\n",
    "def regularized_ols_gradient(A, x, y, lam):\n",
    "    return A.T @ (A @ x - y) + lam * x\n",
    "\n",
    "# BFGS Implementation for Regularized OLSLR\n",
    "def bfgs_regularized_ols(A, y, lam, tol=1e-6, max_iter=100, epsilon=1e-8, regularization=1e-5):\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros((n, 1))  # Starting point\n",
    "    B = np.eye(n)  # Initial Hessian approximation\n",
    "    history = []\n",
    "\n",
    "    # Regularization for improved stability\n",
    "    regularization_matrix = regularization * np.eye(n)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad = regularized_ols_gradient(A, x, y, lam)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "        # Compute search direction\n",
    "        p = -np.linalg.solve(B + regularization_matrix, grad)\n",
    "        x_new = x + p\n",
    "\n",
    "        s = x_new - x\n",
    "        y_grad = regularized_ols_gradient(A, x_new, y, lam) - grad\n",
    "\n",
    "        # Debugging: Print intermediate values\n",
    "        if k % 10 == 0:\n",
    "            print(f\"Iteration {k}\")\n",
    "            print(f\"Gradient Norm: {np.linalg.norm(grad)}\")\n",
    "            print(f\"s.T @ y_grad: {s.T @ y_grad}\")\n",
    "\n",
    "        # Safeguard for small denominators\n",
    "        denom_1 = max(s.T @ y_grad, epsilon)\n",
    "        denom_2 = max(s.T @ (B @ s), epsilon)\n",
    "\n",
    "        # Ensure s and y_grad are column vectors\n",
    "        s = s.reshape(-1, 1)\n",
    "        y_grad = y_grad.reshape(-1, 1)\n",
    "\n",
    "        # Update B using the stabilized BFGS formula\n",
    "        Bs = B @ s\n",
    "        B += np.outer(s, s) / denom_1 - np.outer(Bs, Bs) / denom_2\n",
    "\n",
    "        # Update x and save history\n",
    "        x = x_new\n",
    "        history.append(np.linalg.norm(grad))\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Solve Regularized OLSLR using BFGS with lambda = 0.001\n",
    "print(\"\\nSolving Regularized OLSLR using BFGS (lambda = 0.001)...\")\n",
    "lambda_val = 0.001\n",
    "x_star_f_lambda, history_f_lambda = bfgs_regularized_ols(A, y, lam=lambda_val)\n",
    "print(f\"x_star_f_lambda:\\n{x_star_f_lambda}\")\n",
    "print(f\"Number of iterations: {len(history_f_lambda)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
