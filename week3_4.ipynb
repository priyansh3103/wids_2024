{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of A^T A: inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and scale the dataset\n",
    "digits = load_digits()\n",
    "A = digits.data\n",
    "y = digits.target.reshape(-1, 1)\n",
    "\n",
    "# Standardize the data matrix A\n",
    "scaler = StandardScaler()\n",
    "A = scaler.fit_transform(A)\n",
    "\n",
    "# Compute condition number of A^T A\n",
    "cond_number = np.linalg.cond(A.T @ A)\n",
    "print(f\"Condition number of A^T A: {cond_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Gradient Norm: 5376.846925988702\n",
      "\n",
      "Solution x_star:\n",
      "[[ 0.        ]\n",
      " [ 0.07780044]\n",
      " [-0.04797939]\n",
      " [-0.12024021]\n",
      " [ 0.2495084 ]\n",
      " [-0.02640112]\n",
      " [-0.11531941]\n",
      " [-0.00574959]\n",
      " [ 0.11074914]\n",
      " [-0.0871883 ]\n",
      " [ 0.56089123]\n",
      " [ 0.17486264]\n",
      " [-0.31349658]\n",
      " [-0.44913292]\n",
      " [ 0.31448385]\n",
      " [ 0.1962357 ]\n",
      " [-0.05745679]\n",
      " [ 0.07481687]\n",
      " [ 0.44989254]\n",
      " [-0.18187064]\n",
      " [-0.41647436]\n",
      " [ 0.30009384]\n",
      " [-0.17967787]\n",
      " [-0.11336161]\n",
      " [-0.02997526]\n",
      " [-0.49659961]\n",
      " [ 0.2309326 ]\n",
      " [ 0.49772216]\n",
      " [ 0.44312347]\n",
      " [ 0.55145106]\n",
      " [-0.0709742 ]\n",
      " [-0.1320009 ]\n",
      " [ 0.        ]\n",
      " [-0.54737128]\n",
      " [-0.1300146 ]\n",
      " [ 0.84929873]\n",
      " [-0.27087914]\n",
      " [ 0.20595285]\n",
      " [-0.03645851]\n",
      " [ 0.        ]\n",
      " [ 0.01939488]\n",
      " [ 0.33075962]\n",
      " [-0.11310853]\n",
      " [-0.04258119]\n",
      " [ 0.70463526]\n",
      " [ 0.31787724]\n",
      " [ 0.04000298]\n",
      " [ 0.03019687]\n",
      " [ 0.12299392]\n",
      " [ 0.04775064]\n",
      " [-0.03242147]\n",
      " [-0.34754968]\n",
      " [-1.09457288]\n",
      " [-0.2469263 ]\n",
      " [ 0.5115001 ]\n",
      " [-0.14539175]\n",
      " [-0.02937916]\n",
      " [-0.12940149]\n",
      " [ 0.04231358]\n",
      " [-0.27009546]\n",
      " [-0.03228253]\n",
      " [-0.46725922]\n",
      " [-0.01192364]\n",
      " [-0.10174699]]\n",
      "Number of iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the gradient and Hessian for Regularized OLSLR\n",
    "def regularized_ols_gradient(A, x, y, lam):\n",
    "    return A.T @ (A @ x - y) + lam * x\n",
    "\n",
    "def regularized_ols_hessian(A, lam):\n",
    "    return A.T @ A + lam * np.eye(A.shape[1])\n",
    "\n",
    "# Regularized Newton's Method Implementation\n",
    "def newton_method_stable(A, y, lam, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros((n, 1))  # Starting point\n",
    "    iter_count = 0\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad = regularized_ols_gradient(A, x, y, lam)\n",
    "        hess = regularized_ols_hessian(A, lam)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "        # Newton's update step with regularization\n",
    "        delta_x = np.linalg.solve(hess, -grad)\n",
    "        x += delta_x\n",
    "        iter_count += 1\n",
    "\n",
    "        # Debugging: Print intermediate diagnostics\n",
    "        if k % 10 == 0:\n",
    "            print(f\"Iteration {k}, Gradient Norm: {np.linalg.norm(grad)}\")\n",
    "\n",
    "    return x, iter_count\n",
    "\n",
    "# Solve Regularized OLSLR with lambda = 0.01\n",
    "lambda_val = 0.01\n",
    "x_star, iterations = newton_method_stable(A, y, lam=lambda_val)\n",
    "print(f\"\\nSolution x_star:\\n{x_star}\")\n",
    "print(f\"Number of iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Gradient Norm: 5376.846925988702\n",
      "Iteration 10, Gradient Norm: 7.659315607391443e+74\n",
      "Iteration 20, Gradient Norm: 5.246355676584303e+149\n",
      "Iteration 30, Gradient Norm: nan\n",
      "Iteration 40, Gradient Norm: nan\n",
      "Iteration 50, Gradient Norm: nan\n",
      "Iteration 60, Gradient Norm: nan\n",
      "Iteration 70, Gradient Norm: nan\n",
      "Iteration 80, Gradient Norm: nan\n",
      "Iteration 90, Gradient Norm: nan\n",
      "\n",
      "Solution x_star (BFGS):\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "Number of iterations: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_98429/3588746320.py:23: RuntimeWarning: overflow encountered in matmul\n",
      "  denom_1 = max(s.T @ y_grad, epsilon)\n",
      "/var/folders/kq/vv3n1vnx70b7hcbwwrw4m02r0000gn/T/ipykernel_98429/3588746320.py:23: RuntimeWarning: invalid value encountered in matmul\n",
      "  denom_1 = max(s.T @ y_grad, epsilon)\n"
     ]
    }
   ],
   "source": [
    "# BFGS Implementation with Stability Enhancements\n",
    "def bfgs_stable(A, y, lam, tol=1e-6, max_iter=100, epsilon=1e-8):\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros((n, 1))  # Starting point\n",
    "    B = np.eye(n)  # Initial Hessian approximation\n",
    "    history = []\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad = regularized_ols_gradient(A, x, y, lam)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "        # Compute search direction\n",
    "        p = -np.linalg.solve(B, grad)\n",
    "        x_new = x + p\n",
    "\n",
    "        s = x_new - x\n",
    "        y_grad = regularized_ols_gradient(A, x_new, y, lam) - grad\n",
    "\n",
    "        # Safeguard for small denominators\n",
    "        denom_1 = max(s.T @ y_grad, epsilon)\n",
    "        denom_2 = max(s.T @ (B @ s), epsilon)\n",
    "\n",
    "        # Ensure s and y_grad are column vectors\n",
    "        s = s.reshape(-1, 1)\n",
    "        y_grad = y_grad.reshape(-1, 1)\n",
    "\n",
    "        # Update B using the stabilized BFGS formula\n",
    "        Bs = B @ s\n",
    "        B += np.outer(s, s) / denom_1 - np.outer(Bs, Bs) / denom_2\n",
    "\n",
    "        # Update x and save history\n",
    "        x = x_new\n",
    "        history.append(np.linalg.norm(grad))\n",
    "\n",
    "        # Debugging: Print diagnostics\n",
    "        if k % 10 == 0:\n",
    "            print(f\"Iteration {k}, Gradient Norm: {np.linalg.norm(grad)}\")\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Solve Regularized OLSLR with BFGS (lambda = 0.01)\n",
    "lambda_val = 0.01\n",
    "x_star_bfgs, history_bfgs = bfgs_stable(A, y, lam=lambda_val)\n",
    "print(f\"\\nSolution x_star (BFGS):\\n{x_star_bfgs}\")\n",
    "print(f\"Number of iterations: {len(history_bfgs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
